# MoeChat 记忆系统详解

MoeChat 的记忆系统是其核心竞争力之一，它通过区分不同类型的“记忆体”来模拟更接近人类的交互体验，避免了传统基于单一向量数据库的 LLM 应用在长期对话中容易出现的记忆混淆、时间感知模糊等问题。

## 核心亮点

1.  **模块化设计**：将记忆功能分解为 **短期记忆（上下文）**、**长期记忆（日记系统）**、**核心记忆（个人资料）** 和 **世界书（知识库）** 四个独立模块。
2.  **精准的时间感知与查询**：特别是长期记忆系统，它不依赖传统的向量相似度搜索，而是结合时间实体提取和语义检索，能够精确响应如“昨天我们聊了什么？”、“上周你吃了什么？”这样的基于时间的查询。
3.  **分层检索与融合**：在每次对话中，系统会并行检索多个记忆模块，然后将检索到的相关信息（如果满足阈值）作为额外的上下文注入到最终发送给 LLM 的提示词中。
4.  **自动化与智能化**：核心记忆和长期记忆的更新由 LLM 自动完成。系统会从对话中提取关键信息来扩展记忆，无需人工干预。

## 记忆模块详解与协作机制

### 1. 短期记忆 (Short-Term Memory / 上下文 Context)

*   **实现位置**: `utilss/agent.py` 中的 `Agent` 类。
*   **存储**: `self.msg_data` (主上下文) 和 `self.msg_data_tmp` (临时上下文) 列表。
*   **作用**:
    *   维持最基本的对话连贯性。
    *   作为 LLM 推理的直接输入。
*   **管理**:
    *   每次用户输入和助手回复都会被添加到 `self.msg_data_tmp`。
    *   在助手回复处理完成后，`self.msg_data_tmp` 会追加到 `self.msg_data`。
    *   `self.msg_data` 会根据 `config.yaml` 中 `Agent.context_length` 的设置进行截断，保留最新的 N 条记录。
    *   会持久化保存到 `./data/agents/{char}/history.yaml` 文件中。
*   **特点**:
    *   实时、易失（超出长度限制会被丢弃）。
    *   直接用于 LLM 推理。

### 2. 长期记忆 (Long-Term Memory / 日记系统)

*   **实现位置**: `utilss/long_mem.py` 中的 `Memorys` 类。
*   **存储**:
    *   **文件**: 以日期分割存储在 `./data/agents/{char}/memorys/` 目录下的 `.yaml` 文件中。
    *   **内容**: 每条记录包含完整对话 (`msg`) 和一个用于检索的摘要标签 (`text_tag`)。
    *   **索引**: 摘要标签被向量化并存储在 `.pkl` 文件中（使用 pickle），内存中维护 `self.memorys_key` (排序的时间戳列表)、`self.memorys_data` (对话内容字典) 和 `self.vectors` (标签向量列表)。
*   **工作机制**:
    *   **信息提取 (记录)**:
        *   助手回复后，后台线程 `Memorys.add_memory1` 被触发。
        *   调用 LLM (使用 `prompt.get_mem_tag_prompt`) 分析用户上一条消息，提取关键信息，生成一个简洁的摘要标签 (e.g., "用户说吃了火锅") 或 "日常闲聊"。
    *   **存储**:
        *   将用户消息、助手回复、时间戳和摘要标签存入对应日期的 `.yaml` 文件。
        *   对摘要标签进行向量化，并存入 `.pkl` 文件。
        *   更新内存中的 `self.memorys_key`, `self.memorys_data`, `self.vectors`。
    *   **检索**:
        *   用户输入新消息时，后台线程 `Memorys.get_memorys` 被触发。
        *   使用 `jionlp` 解析输入中的时间实体 (e.g., "昨天", "上周三")，计算出绝对时间范围。
        *   利用内存中的排序时间戳列表 `self.memorys_key` 和 `bisect` 算法，快速定位该时间范围内的所有记忆条目索引。
        *   **语义增强检索 (可选)** (`Agent.is_check_memorys` 为 true):
            *   对用户当前输入进行向量化。
            *   遍历上一步定位到的记忆条目的向量，计算与输入向量的点积相似度。
            *   仅保留相似度高于 `Agent.mem_thresholds` 的记忆内容。
        *   **直接范围检索 (默认)** (`Agent.is_check_memorys` 为 false):
            *   直接合并上一步定位到的所有时间范围内的记忆内容。
    *   **注入**:
        *   检索到的相关记忆会被格式化 (使用 `prompt.long_mem_prompt`) 并作为 `system` 消息注入最终提示词。
*   **特点**:
    *   **时间精准**: 通过时间实体解析和 `bisect` 索引，能快速定位特定时间段的记忆。
    *   **内容相关 (可选)**: 通过语义检索 (`is_check_memorys`) 进一步过滤，确保注入的是与当前话题相关的记忆。
    *   **持久化**: 以结构化文件形式存储。
    *   **自动更新**: 对话结束后自动记录。

### 3. 核心记忆 (Core Memory)

*   **实现位置**: `utilss/core_mem.py` 中的 `Core_Mem` 类。
*   **存储**:
    *   **文件**: `./data/agents/{char}/core_mem.yml`。
    *   **内容**: 关于用户的关键事实 (如爱好、住址、重要事件等)。
    *   **索引**: 使用 FAISS (`faiss.IndexFlatIP`) 建立向量索引，内存中维护 `self.mems` (原始文本列表)、`self.msgs` (带时间戳的格式化文本列表) 和 FAISS 索引 `self.index`。
*   **工作机制**:
    *   **信息提取 (记录)**:
        *   助手回复后，后台线程 `Agent.insert_core_mem` 被触发。
        *   调用 LLM (使用 `prompt.get_core_mem`) 分析最近几轮的完整对话。
        *   提取关于 **用户** 的、值得长期记住的关键信息 (e.g., "用户讨厌吃香菜")。
        *   与已有的核心记忆 (`self.mems[-100:]`) 对比，避免重复。
    *   **存储**:
        *   将新提取的核心记忆条目追加写入 `core_mem.yml` 文件。
        *   同时将新的记忆条目向量化，并添加到内存中的 FAISS 索引里。
    *   **检索**:
        *   用户输入新消息时，后台线程 `Core_mem.find_mem` 被触发。
        *   对用户当前输入进行向量化。
        *   使用 FAISS 索引搜索最相似的 Top-K (代码中为 5) 个核心记忆条目。
        *   计算相似度得分，仅返回得分高于阈值 (代码中硬编码为 0.5) 的条目。
    *   **注入**:
        *   检索到的相关核心记忆会被格式化 (使用 `prompt.core_mem_prompt`) 并作为 `system` 消息注入最终提示词。
*   **特点**:
    *   **语义检索**: 专注于用户相关的静态或半静态信息。
    *   **高价值**: 存储用户画像中最核心的信息。
    *   **持久化**: 以结构化文件形式存储。
    *   **自动更新**: 对话结束后自动分析并记录。

### 4. 世界书 (World Book / 知识库)

*   **实现位置**: `utilss/data_base.py` 中的 `DataBase` 类。
*   **存储**:
    *   **文件**: 一系列 `.yaml` 格式文件，存放在 `./data/agents/{char}/data_base/` 目录下。
    *   **格式**: 每个文件包含多个 "关键词" -> "详细描述" 的映射。
    *   **索引**: 使用 FAISS 建立向量索引。启动时加载所有 `.yaml` 文件，对 "关键词" 向量化，将向量和 "详细描述" 存入 FAISS 索引。通过 MD5 校验实现缓存 (`.pkl` 文件)。
*   **工作机制**:
    *   **加载与索引**:
        *   系统启动时或检测到 `.yaml` 文件变更时，重新加载并建立 FAISS 索引。
        *   对每个 "关键词" 进行向量化。
        *   将向量和对应的 "详细描述" 存入 FAISS 索引。
    *   **检索**:
        *   用户输入新消息时，后台线程 `Agent.get_data` (内部调用 `DataBase.search`) 被触发。
        *   使用 `jionlp.split_sentence` 将输入分割成短句。
        *   对每个短句进行向量化。
        *   使用 FAISS 索引搜索最相似的 Top-K (`Agent.scan_depth`) 个知识条目。
        *   计算相似度得分，仅返回得分高于 `Agent.books_thresholds` 的条目。
    *   **注入**:
        *   检索到的相关知识会被格式化 (使用 `prompt.data_base_prompt`) 并作为 `system` 消息注入最终提示词。
*   **特点**:
    *   **静态知识**: 存储角色扮演所需的世界观、人物、物品、规则等背景知识。
    *   **语义检索**: 通过 FAISS 实现快速、高效的语义匹配。
    *   **可配置**: 通过 `scan_depth` 和 `books_thresholds` 控制检索的广度和精度。
    *   **模块化**: 通过不同的 `.yaml` 文件组织知识。

## 协作流程与快速响应

当用户与 AI 角色进行一次对话时，协作流程如下：

### 对话处理与记忆检索流程图 (ASCII Art)

```
+-------------------------+
| 1. 用户发送消息 msg     |
+-------------------------+
            |
            v
+-------------------------+
| 2. Agent.get_msg_data() |
|   - 将 msg 加入临时上下文 |
+-------------------------+
            |
            v
+---------------------------------------------+
| 3. 启动并行后台检索线程                      |
|  +----------------+  +------------------+   |
|  | 查询世界书      |  | 查询长期记忆      |  |
|  | (DataBase.search)| | (Memorys.get_memorys)| |
|  +----------------+  +------------------+   |
|  +----------------+                         |
|  | 查询核心记忆    |                         |
|  | (Core_mem.find_mem)|                      |
|  +----------------+                         |
+---------------------------------------------+
            |
            v
+--------------------------------+
| 4. 等待所有检索线程完成         |
+--------------------------------+
            |
            v
+---------------------------------------------+
| 5. 检查各模块检索结果            |
|  +--------+  +--------+  +--------+          |
|  | 世界书? |  | 核心记忆?|  | 长期记忆? |         |
|  +--------+  +--------+  +--------+          |
|       |         |          |                |
|       v         v          v                |
|  +--------+  +--------+  +--------+          |
|  | 格式化  |  | 格式化  |  | 格式化  |         |
|  | 并注入  |  | 并注入  |  | 并注入  |         |
|  +--------+  +--------+  +--------+          |
+---------------------------------------------+
            |
            v
+----------------------------------+
| 6. 注入当前时间戳到提示词         |
+----------------------------------+
            |
            v
+--------------------------------------------------+
| 7. 组合最终提示词发送给 LLM                       |
|   (角色设定 + 示例 + 检索到的记忆/知识 +          |
|    主上下文 + 临时上下文)                          |
+--------------------------------------------------+
            |
            v
+-------------------------+
| 8. 调用 LLM API 获取回复 |
+-------------------------+
            |
            v
+-------------------------+
| 9. Agent.add_msg()      |
|   - 将回复加入临时上下文 |
+-------------------------+
            |
            v
+---------------------------------------------+
| 10. 启动后台更新线程                         |
|  +-----------------------+  +--------------+ |
|  | 提取并更新核心记忆     |  | 记录长期记忆  | |
|  | (insert_core_mem)     |  | (add_memory1) | |
|  +-----------------------+  +--------------+ |
+---------------------------------------------+
            |
            v
+----------------------------------+
| 11. 更新主上下文并持久化历史记录 |
|   - 更新 self.msg_data           |
|   - 截断上下文                   |
|   - 写入 history.yaml            |
|   - 清空临时上下文               |
+----------------------------------+
```

### 快速响应的关键

*   **并行处理**: 世界书、长期记忆、核心记忆的检索是并行进行的，缩短了总检索时间。
*   **高效索引**:
    *   长期记忆通过时间戳排序和 `bisect` 实现 O(log N) 的快速范围查找。
    *   核心记忆和世界书通过 FAISS 向量索引实现近实时的语义检索。
*   **分层检索**:
    *   长期记忆先按时间快速缩小范围，再可选地进行语义过滤。
    *   核心记忆和世界书先通过 FAISS 快速检索 Top-K，再按阈值过滤。
*   **后台更新**: 记忆的记录和更新操作（如调用 LLM 提取信息）放在后台线程执行，不阻塞主对话流程。

总结来说，MoeChat 通过将不同类型的记忆和知识分而治之，并利用高效的检索技术和并行处理机制，构建了一个既强大又快速响应的记忆系统，极大地提升了 AI 角色扮演的真实感和智能水平。